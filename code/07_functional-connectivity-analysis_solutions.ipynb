{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Connectivity Analysis\n",
    "***\n",
    "\n",
    "Now we have an idea of three important components to analyzing neuroimaging data:\n",
    "\n",
    "1. Data manipulation\n",
    "2. Cleaning and confound regression\n",
    "3. Parcellation and signal extraction\n",
    "\n",
    "In this notebook the goal is to integrate these 3 basic components and perform a full analysis of group data using **Intranetwork Functional Connectivity (FC)**. \n",
    "\n",
    "Intranetwork functional connectivity is essentially a result of performing correlational analysis on mean signals extracted from two ROIs. Using this method we can examine how well certain resting state networks, such as the **Default Mode Network (DMN)**, are synchronized across spatially distinct regions. \n",
    "\n",
    "ROI-based correlational analysis forms the basis of many more sophisticated kinds of functional imaging analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "***\n",
    "\n",
    "The outline of the notebook is divided into two parts. The first part directly uses what you've learned and builds upon it to perform the final functional connectivity analysis on group data. \n",
    "\n",
    "The second part shows how we can use Nilearn's convenient wrapper functionality to perform the same task with *significantly less effort*. \n",
    "\n",
    "#### Part A: Manual computation \n",
    "1. Functional data cleaning and confound regression\n",
    "2. Applying a parcellation onto the data\n",
    "3. Computing the correlation between two ROI time-series\n",
    "\n",
    "\n",
    "#### Part B: Using Nilearn's high-level features\n",
    "1. Using NiftiLabelsMasker to extract cleaned time-series\n",
    "2. Computing the correlation between two ROI time-series\n",
    "3. Performing analysis on all subjects\n",
    "4. Visualization of final results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Assignment\n",
    "\n",
    "The first couple of bits in this notebook are a review of everything you've covered in the course so far! To solidify the concepts you've learned so far we've coverted this notebook into a guided assignment!\n",
    "\n",
    "If you have questions we recommend you take the following steps ***before asking an instructor!***\n",
    "\n",
    "**Forgot how to do something?**\n",
    "- Review previous notebooks, these contain most of the information you'll need to do assignments\n",
    "\n",
    "**Don't know how a function works?**\n",
    "- Use `SHIFT-TAB` after opening a bracket to view the help documentation\n",
    "- Use the power of `Google` and just type in the function, you will almost definitely find the documentation page for that function!\n",
    "- Refer to Nilearn's documentation: https://nilearn.github.io/modules/reference.html. This contains information about *all* of nilearn's functions!\n",
    "\n",
    "**Running into an error? Here are some debugging tips for you!**\n",
    "\n",
    "1. If you run into an issue, *always check your variables!* It may be that you:\n",
    "    a. Forgot to set a variable (\"variable\" not defined error)\n",
    "    b. There was a typo somewhere earlier in the code (i.e if the variable is the wrong size, or is a list with no elements)\n",
    "    c. If you're dealing with images or arrays, then using the `.shape` property is essential to knowing whether your data is what you expect it to be!\n",
    "    \n",
    "**Still need help figuring it out?**\n",
    "\n",
    "Ask in the chat! \n",
    "\n",
    "Feel free to answer other people's questions, we (the instructors) will step in if things get too complicated!    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as nimg\n",
    "from nilearn import plotting as nplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bids import BIDSLayout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pick a single subject\n",
    "fmriprep_dir = '../data/ds000030/derivatives/fmriprep'\n",
    "sub = '10788'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "BIDS root does not exist: /Users/swalkow2/Documents/GitHub/SDC-BIDS-fMRI/data/ds000030/derivatives/fmriprep",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/02/bsg_wyl53q1bq10qyh56b_q40000gp/T/ipykernel_2951/865981971.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Use PyBIDS to parse BIDS data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBIDSLayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmriprep_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/SDC-BIDS-fMRI/CDS-BIDS-fMRI/lib/python3.9/site-packages/bids/layout/layout.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, validate, absolute_paths, derivatives, config, sources, ignore, force_index, config_filename, regex_search, database_path, database_file, reset_database, index_metadata)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;31m# Do basic BIDS validation on root directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_root\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/GitHub/SDC-BIDS-fMRI/CDS-BIDS-fMRI/lib/python3.9/site-packages/bids/layout/layout.py\u001b[0m in \u001b[0;36m_validate_root\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"BIDS root does not exist: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dataset_description.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: BIDS root does not exist: /Users/swalkow2/Documents/GitHub/SDC-BIDS-fMRI/data/ds000030/derivatives/fmriprep"
     ]
    }
   ],
   "source": [
    "#Use PyBIDS to parse BIDS data structure\n",
    "layout = BIDSLayout(fmriprep_dir, validate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `layout.get` to pull the MNI152NLin2009cAsym files as well as the confound tsv file! \n",
    "\n",
    "Remember the confound tsv file doesn't have a \"space\" associated with it!\n",
    "\n",
    "**HINT**:\n",
    "Use `return_type=\"file\"` so that you get a list of file paths rather than a list of \"BIDS\" file objects. \n",
    "\n",
    "If you don't use `return_type=\"file\"`, you may need to use `.path` after pulling an element from the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get resting state data (preprocessed, mask, and confounds file)\n",
    "func_files = layout.get(subject=sub, datatype='func', task='rest',  suffix='preproc', space='MNI152NLin2009cAsym', return_type='file')\n",
    "mask_files = layout.get(subject=sub, datatype='func', task='rest', suffix='brainmask', space='MNI152NLin2009cAsym', return_type='file')\n",
    "confound_files = layout.get(subject=sub, datatype='func', task='rest', suffix='confounds', return_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note that when we use return_type='file' when using <code>layout.get</code> we no longer need to use <code>confound_files.path</code>. We could technically use this on all the files!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select MNI files\n",
    "func_file = func_files[0]\n",
    "mask_file = mask_files[0]\n",
    "confound_file = confound_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Manual Computation of Functional Connectivity\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cleaning your functional data using filtering, dummy TR removal and confound regression\n",
    "The first step to any functional analysis is to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to help extract our confound regressors from the .tsv file for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to part_06 for code + explanation\n",
    "def extract_confounds(confound_tsv,confounds,dt=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        confound_tsv                    Full path to confounds.tsv\n",
    "        confounds                       A list of confounder variables to extract\n",
    "        dt                              Compute temporal derivatives [default = True]\n",
    "        \n",
    "    Outputs:\n",
    "        confound_mat                    \n",
    "    '''\n",
    "    \n",
    "    #Load in data using Pandas then extract relevant columns\n",
    "    confound_df = pd.read_csv(confound_tsv,delimiter='\\t') \n",
    "    confound_df = confound_df[confounds]\n",
    "    \n",
    "    #If using temporal derivatives \n",
    "    if dt:\n",
    "        #For each column create a new column '<colname>_dt' containing the step-wise differences\n",
    "        for col in confound_df.columns:\n",
    "            confound_df['{}_dt'.format(col)] = confound_df[col].diff() \n",
    "    \n",
    "    #Convert into a matrix of values (timepoints)x(variable)\n",
    "    confound_mat = confound_df.values \n",
    "    \n",
    "    #Return confound matrix\n",
    "    return confound_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Let's clean our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings to use:\n",
    "- Confounders: X, Y, Z, RotX, RotY, RotZ, aCompCor01, aCompCor02, Global Signal\n",
    "- Temporal Derivatives: Yes\n",
    "- high_pass = 0.009\n",
    "- low_pass = 0.08\n",
    "- detrend = True\n",
    "- standardize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load functional image\n",
    "tr_drop = 4\n",
    "func_img = nimg.load_img(func_file)\n",
    "func_img = func_img.slicer[:,:,:,tr_drop:]\n",
    "\n",
    "#Extract confounds\n",
    "confounds = extract_confounds(confound_file,['X','Y','Z','RotX','RotY','RotZ','GlobalSignal',\n",
    "                                            'aCompCor01','aCompCor02'])\n",
    "confounds = confounds[tr_drop:,:] \n",
    "\n",
    "#Clean functional image\n",
    "clean_img = nimg.clean_img(func_img,confounds=confounds,low_pass=0.08,high_pass=0.009,t_r=2,\n",
    "                         mask_img=mask_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting up the parcellation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a parcellation we'll have to specify a parcellation to use. \n",
    "For this analysis we'll be using a spatially separated version [Yeo 2011 - 7 Networks](https://www.ncbi.nlm.nih.gov/pubmed/21653723).\n",
    "\n",
    "We chose this parcellation since it nicely characterizes the **DMN**, our network of interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the original Yeo7 parcellation\n",
    "net7 = '../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_7Networks_MNI152_FreeSurferConformed1mm_LiberalMask.nii.gz'\n",
    "nplot.plot_roi(net7,cmap='Paired',colorbar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the spatially separated Yeo 2011 7 networks and view\n",
    "\n",
    "parcel_file = '../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/relabeled_yeo_atlas.nii.gz' \n",
    "yeo_7 = nimg.load_img(parcel_file)\n",
    "nplot.plot_roi(yeo_7,cmap='Paired',colorbar=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to use the parcellation with our functional data it must have the same dimensions. It turns out that the parcellation schema has slightly different dimensions, so we need to resample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use \"nearest\" to preserve the label values (a label of 2.42 for example, doesn't and shouldn't exist) \n",
    "resamp_yeo7 = nimg.resample_to_img(yeo_7,clean_img,interpolation='nearest') \n",
    "print(resamp_yeo7.shape)\n",
    "print(clean_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intra-network functional connectivity analysis\n",
    "Intra-network functional connectivity analysis is determined by computing the correlation between the mean time-series of two spatially distinct regions within the same network. \n",
    "\n",
    "To perform this analysis requires a few simple steps:\n",
    "1. Select 2 ROIs from the same network (DMN) \n",
    "2. Extract the mean time-series from both regions \n",
    "3. Compute the correlation between the two mean ROI time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to select two ROIs from the DMN. We've already gone through the hassle of selecting these two regions but many possible combinations exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select two ROIs and visualize\n",
    "source_ROI = 44\n",
    "target_ROI = 46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the ROI, we can do this by masking our all values not matching our ROI\n",
    "source_mask = nimg.math_img('a == {}'.format(source_ROI), a=resamp_yeo7) \n",
    "\n",
    "#Apply the mask to the parcellation atlas (resamp_yeo7)\n",
    "masked_source = nimg.math_img('a*b',a=resamp_yeo7,b=source_mask) \n",
    "\n",
    "#Visualize\n",
    "nplot.plot_roi(masked_source) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mask = nimg.math_img('a == {}'.format(target_ROI),a=resamp_yeo7)\n",
    "masked_target = nimg.math_img('a*b',a=resamp_yeo7,b=target_mask)\n",
    "nplot.plot_roi(masked_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our two regions selected, we'll now extract the mean time-series for each of our two ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall that in the parcellation, each voxel is labelled with a number corresponding to a distinct parcel\n",
    "#We want to extract parcels belonging to our ROI.\n",
    "\n",
    "#Pull the voxels indices belonging to the ROI \n",
    "yeo7_data = resamp_yeo7.get_data() \n",
    "\n",
    "#Get voxel coordinates (x,y,z) list of source and target ROIs\n",
    "source_roi = np.where(yeo7_data == source_ROI)\n",
    "target_roi = np.where(yeo7_data == target_ROI)\n",
    "\n",
    "#Load up functional data to extract ROI voxels from\n",
    "func_data = clean_img.get_data()\n",
    "\n",
    "#Extract the list of voxel time-series belonging to each ROI\n",
    "#This is now a (roi voxel)x(timepoints) array\n",
    "source_ts = func_data[source_roi]\n",
    "target_ts = func_data[target_roi] \n",
    "\n",
    "#We want to compute the mean timeseries of each list of voxels (source and target) \n",
    "#This will be a (1) x (timepoints) vector\n",
    "mean_source_ts = np.mean(source_ts,axis=0)\n",
    "mean_target_ts = np.mean(target_ts,axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've extracted two average time-series, one from the source, and one from the target.\n",
    "The last step is to compute the correlation between them. This will tell us how well the average time-series from the two DMN regions synchronize with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guide to complete these cells**\n",
    "\n",
    "In the above cell we calculated for each of our ROIs a mean time-series signal in the form of `mean_source_ts` and `mean_target_ts`. First let's check the `shape` of these time-series (1-D arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of mean_source_ts and mean_target_ts\n",
    "print(mean_source_ts.shape)\n",
    "print(mean_target_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 1-D arrays of the same shape!\n",
    "\n",
    "\n",
    "To calculate the **Functional Connectivity** of our two regions of interest we need to calculate the correlation between our source ROI, `mean_source_ts` and our target `mean_target_ts`\n",
    "\n",
    "To do this we use <code>np.corrcoef(x,y)</code> which returns a matrix of form:\n",
    "\n",
    "$$ \\rho=\n",
    "\\left( \\begin{matrix}\n",
    "\\rho_{1,1} & \\rho_{1,2} \\\\\n",
    "\\rho_{2,1} & \\rho_{2,2}\n",
    "\\end{matrix} \\right)\n",
    "$$\n",
    "\n",
    "The diagonals represent the correlation of signals with themselves. These are always $1$. The off-diagonal represents the correlation of one signal with another is exactly what we want. In addition the matrix is symmetric so: $\\rho_{1,2} = \\rho_{2,1}$. \n",
    "\n",
    "Run the following cell to get the documentation of np.corrcoef!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now implement `np.corrcoef` to calculate the correlation coefficients of the `mean_source_ts` and `mean_target_ts`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute correlation and pull the value in the first row, second column\n",
    "correlation_matrix = np.corrcoef(mean_source_ts,mean_target_ts)\n",
    "print(\"The shape of the matrix is:\", correlation_matrix.shape)\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice our correlation matrix is a 2x2 array! Which one of the 4 values is the connectivity/correlation between our two regions of interest?\n",
    "\n",
    "Refer to the text above to get a hint about which value to look at! We can pick and choose values from our matrix using 2 indices:\n",
    "\n",
    "`correlation_matrix[i,j]`\n",
    "\n",
    "Where `i` refers to which row to look at and `k` refers to which column to look at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity = correlation_matrix[0,1]\n",
    "print(connectivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Completed!\n",
    "\n",
    "Hopefully **Part A served as good review toward re-familiarizing yourself with parcellations/cleaning/data manipulation**. In addition it turns out that Functional Connectivity is really *just a correlation between two time-series signals*!\n",
    "\n",
    "In the next section, we'll go over how `nilearn` greatly simplifies all the steps you just did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Using nilearn's label masker to extract the timeseries\n",
    "***\n",
    "nilearn has a built in function for extracting timeseries from functional files and doing all the extra signal processing at the same time! It's everything you just did, **all in one step!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import a package from <code>nilearn</code>, called <code>input_data</code> which allows us to pull data using the parcellation file, and at the same time applying data cleaning!\n",
    "\n",
    "We first create an object using the parcellation file <code>yeo_7</code> and our cleaning settings which are the following:\n",
    "\n",
    "Settings to use:\n",
    "- Confounders: X, Y, Z, RotX, RotY, RotZ, aCompCor01, aCompCor02, Global Signal\n",
    "- Temporal Derivatives: Yes\n",
    "- high_pass = 0.009\n",
    "- low_pass = 0.08\n",
    "- detrend = True\n",
    "- standardize = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=yeo_7,\n",
    "                                      standardize=True,\n",
    "                                      memory='nilearn_cache',\n",
    "                                      verbose=1,\n",
    "                                      detrend=True,\n",
    "                                     low_pass = 0.08,\n",
    "                                     high_pass = 0.009,\n",
    "                                     t_r=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object <code>masker</code> is now able to be used on *any functional image of the same size*. What it means to *use the masker* is that you can automatically *apply a parcellation and extract data at the same time*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn's ConnectivityMeasure to calculate our correlation matrix\n",
    "\n",
    "The second step is to compute the functional connectivity (correlation) matrix. When we use <code>masker</code>, we can compute the correlation matrix *between all ROIs in our parcellation atlas at the same time*. Below we'll show an example on how to use the <code>masker</code> in order to compute correlations on our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll automatically clean and average data for each of our ROIs at the same time. This is done using <code>masker.fit_transform</code>:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guide**\n",
    "\n",
    "Remember to hit `Shift-TAB` while the cursor is inside the brackets in order to view the help dialog for the function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_and_averaged_time_series = masker.fit_transform(func_img, confounds)\n",
    "cleaned_and_averaged_time_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, this data is *automatically parcellated for you, and, in addition, is cleaned using the confounds you've specified already!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using this data we can calculate a *full correlation matrix* - this is the correlation between *all pairs of ROIs* in our parcellation scheme! We'll use another nilearn tool called <code>ConnectivityMeasure</code> from <code>nilearn.connectome</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the masker, we need to make an object that will calculate connectivity for us.\n",
    "\n",
    "**Recall from the earlier section: Functional connectivity is just a ________?** \n",
    "\n",
    "**Also `SHIFT-TAB` to see what options you can put into the `kind` argument of `ConnectivityMeasure`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_measure = ConnectivityMeasure(kind='correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use <code>correlation_measure.fit_transform()</code> in order to calculate the full correlation matrix for our parcellated data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_correlation_matrix = correlation_measure.fit_transform([cleaned_and_averaged_time_series])\n",
    "full_correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note that we're using a list <code>[cleaned_and_averaged_time_series]</code>, this is becasue <code>correlation_measure</code> works on a *list of subjects*. We'll take advantage of this later!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix which has:\n",
    "\n",
    "- A number of rows matching the number of ROIs in our parcellation atlas\n",
    "- A number of columns, that also matches the number of ROIs in our parcellation atlas\n",
    "\n",
    "You can read this correlation matrix as follows:\n",
    "\n",
    "- Suppose we wanted to know the correlation between ROI 30 and ROI 40\n",
    "- Then Row 30, Column 40 gives us this correlation. \n",
    "- Row 40, Column 40 can also give us this correlation\n",
    "\n",
    "This is because the correlation of $A \\to B = B \\to A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Now try calculating correlation matrices for all subjects in our dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've provided some skeleton code to handle some of the logic. Try to fill in the blanks to the best of your ability!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that here we're using a Python for-loop to go over *all the subjects*!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_subjects = []\n",
    "ctrl_subjects = []\n",
    "schz_subjects = []\n",
    "\n",
    "for sub in subjects:\n",
    "    func_file = layout.get(subject=sub, datatype='func', task='rest',\n",
    "                           suffix='preproc', return_type='file')[0]\n",
    "    \n",
    "    confound_file=layout.get(subject=sub, datatype='func', task='rest',\n",
    "                             suffix='confounds', return_type='file')[0]\n",
    "    \n",
    "    func_img = nimg.load_img(func_file)\n",
    "    func_img = func_img.slicer[:,:,:,tr_drop:]\n",
    "    \n",
    "    confounds = extract_confounds(confound_file,\n",
    "                                 ['X','Y','Z',\n",
    "                                 'RotX','RotY','RotZ',\n",
    "                                 'GlobalSignal','aCompCor01',\n",
    "                                 'aCompCor02'])\n",
    "    \n",
    "    confounds = confounds[tr_drop:,:]\n",
    "    \n",
    "    time_series = masker.fit_transform(func_img,confounds)\n",
    "    pooled_subjects.append(time_series)\n",
    "    \n",
    "    #If the subject ID starts with a \"1\" then they are control\n",
    "    if sub.startswith('1'):\n",
    "        ctrl_subjects.append(time_series)\n",
    "    #If the subejct ID starts with a \"5\" then they are case (case of schizophrenia)\n",
    "    if sub.startswith('5'):\n",
    "        schz_subjects.append(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of all of this code is that:\n",
    "\n",
    "1. Subjects who start with a \"1\" in their ID, are controls, and are placed into the `ctrl_subjects` list\n",
    "2. Subjects who start with a \"2\" in their ID, have schizophrenia, and are placed into the `schz_subjects` list\n",
    "\n",
    "What's actually being placed into the list? The cleaned, parcellated time series data for each subject!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful trick is that we can re-use the <code>correlation_measure</code> object we made earlier and apply it to a *list of subject data*! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_correlation_matrices = correlation_measure.fit_transform(ctrl_subjects)\n",
    "schz_correlation_matrices = correlation_measure.fit_transform(schz_subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have correlation matrices for each subject across two populations. The final step is to examine the differences between these groups in their correlation between ROI 43 and ROI 45:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrices and Group Differences\n",
    "\n",
    "An important step in any analysis is visualizing the data that we have. We've cleaned data, averaged data and calculated correlations but we don't actually know what it looks like! Visualizing data is important to ensure that we don't throw pure nonsense into our final statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize data we'll be using a python package called <code>seaborn</code> which will allow us to create statistical visualizations pretty easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a single subject's correlation matrix by using <code>seaborn</code>'s <code>heatmap</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(ctrl_correlation_matrices[0], cmap='RdBu_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pull our ROI 44 and 46 by indexing our list of correlation matrices as if it were a 3D array (kind of like an MR volume). Take a look at the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ctrl_correlation_matrices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of form:\n",
    "\n",
    "```python\n",
    "ctrl_correlation_matrices[subject_index, row_index, column_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try pulling out:\n",
    "\n",
    "- All subjects\n",
    "- Only row 43 (corresponding to ROI 44)\n",
    "- Only column 45 (corresponding to ROI 46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_roi_vec = ctrl_correlation_matrices[:,43,45]\n",
    "schz_roi_vec = schz_correlation_matrices[:,43,45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn visualization is best achieved using a Pandas dataframe (much like R's dataframes). A dataframe is essentially a spreadsheet-like table and is easily created using numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create control dataframe\n",
    "ctrl_df = pd.DataFrame(data={'dmn_corr':ctrl_roi_vec, 'group':'control'})\n",
    "ctrl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schizophrenia dataframe\n",
    "scz_df = pd.DataFrame(data={'dmn_corr':schz_roi_vec, 'group' : 'schizophrenia'})\n",
    "scz_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "\n",
    "- `ctrl_df` a table containing the correlation value for each control subject, with an additional column with the group label, which is 'control'\n",
    "\n",
    "- `scz_df` a table containing the correlation value for each schizophrenia group subject, with an additional column with the group label, which is 'schizophrenia'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization we're going to stack the two tables together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stack the two dataframes together\n",
    "df = pd.concat([ctrl_df,scz_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NBVAL_SKIP\n",
    "df.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we're going to visualize the results using the python package `seaborn`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize results\n",
    "\n",
    "# Create a figure canvas of equal width and height\n",
    "plot = plt.figure(figsize=(5,5))\n",
    "                  \n",
    "# Create a box plot, with the x-axis as group\n",
    "#the y-axis as the correlation value\n",
    "ax = sns.boxplot(x='group',y='dmn_corr',data=df,palette='Set3')\n",
    "\n",
    "# Create a \"swarmplot\" as well, you'll see what this is..\n",
    "ax = sns.swarmplot(x='group',y='dmn_corr',data=df,color='0.25')\n",
    "\n",
    "# Set the title and labels of the figure\n",
    "ax.set_title('DMN Intra-network Connectivity')\n",
    "ax.set_ylabel(r'Intra-network connectivity $\\mu_\\rho$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing correlations was simple, but interpreting results... \n",
    "# \"Science is hard\" - Colin Hawco"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fe2689612c0ab7397cf4462f922098fb75a2e15e2b422cd69da130ba4292710"
  },
  "jupytext": {
   "formats": "code///ipynb,_episodes///md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
