{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Introduction to functional data cleaning using nilearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movement is the enemy of neuroimagers\n",
    "\n",
    "In task-based fMRI, if participants move during task-relevant moments\n",
    "- Get a huge **false task-related signal** thatâ€™s actually due to motion!\n",
    "\n",
    "In resting-state fMRI, movement can induce **false correlations** between brain regions\n",
    "\n",
    "\n",
    "Solving for this involves *modelling* our fMRI signal to be comprised of **true brain signal** and **confounder signals**. \n",
    "\n",
    "\n",
    "Our goal is to remove a majority (hopefully) of the **confounder signals** and acquire something *closer* to the **true signal**. \n",
    "\n",
    "This is achieved via **confound regression**, which is essentially fitting a linear model using confounds as regressors then subtracting it out from the signal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/.pyenv/versions/3.6.0/envs/scwg_neuroimaging/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nilearn import image as img\n",
    "from nilearn import plotting as plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import bids\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Implementing Confound Regression using FMRIPREP outputs\n",
    "FMRIPREP estimates confounds for the functional image and outputs it into:\n",
    "\n",
    "**sub-xxxxx_task-xxxx_space-xxxx_..._confounds.tsv**\n",
    "\n",
    "Let's look in the functional directory for a confounds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = '10171'\n",
    "fmriprep_dir = '../data/ds000030/derivatives/fmriprep/'\n",
    "layout = bids.BIDSLayout(fmriprep_dir,validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_files = layout.get(subject=sub, datatype='func', task='rest', suffix='preproc')\n",
    "mask_files = layout.get(subject=sub, datatype='func', task='rest', suffix='brainmask')\n",
    "confound_files = layout.get(subject=sub, datatype='func', task='rest', suffix='confounds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_file = func_files[0].path\n",
    "mask_file = mask_files[0].path\n",
    "confound_file = confound_files[0].path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confounds file is organized like an excel spread-sheet with multiple columns, each for a specific confound.\n",
    "\n",
    "We can view these using pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confound_df = pd.read_csv(confound_file,delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas load and view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column in this DataFrame <code>confound_df</code> represents a specific confound variable that is either estimated directly from head motion during the functional scan or other noise characteristics that may capture noise (non grey-matter signal for example). Each row represents values from a TR/sample. So the number of rows in your <code>confound_df</code> should match the number of TRs you have in the functional MR data. The choice of which confounds to use in functional imaging analysis is a source of large debate. We recommend that you check out these sources for a start:\n",
    "\n",
    "1. https://www.sciencedirect.com/science/article/pii/S1053811917302288#f0005\n",
    "2. https://www.sciencedirect.com/science/article/pii/S1053811917302288\n",
    "\n",
    "For now we're going to replicate the pre-processing (mostly) from the seminal Yeo1000 17-networks paper:\n",
    "\n",
    "https://www.ncbi.nlm.nih.gov/pubmed/21653723"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The (mostly, slightly modified) Yeo 2011 Pre-processing schema\n",
    "\n",
    "#### Confound regressors\n",
    "1. 6 motion parameters (X, Y, Z, RotX, RotY, RotZ) \n",
    "2. Global signal (GlobalSignal)\n",
    "3. 2 Largest Principal components of non-grey matter (aCompCor01, aCompCor02)   \n",
    "\n",
    "This is a total of 9 base confound regressor variables. Finally we add temporal derivatives of each of these signals as well (1 temporal derivative for each), the result is 18 confound regressors.\n",
    "\n",
    "***\n",
    "**Temporal Derivatives** are the changes in values across 2 consecutive samples. It represents change in signal over time. For example, when dealing with the confound variable \"X\", which represents motion along the \"X\" direction, the temporal derivative represents *velocity in the X direction*. \n",
    "\n",
    "***\n",
    "\n",
    "#### Low/High pass filtering\n",
    "1. Low pass filtering cutoff: 0.08 \n",
    "2. High pass filtering cutoff: 0.009\n",
    "\n",
    "Low pass filters out high frequency signals from our data. fMRI signals are slow evolving processes, any high frequency signals are likely due to noise \n",
    "High pass filters out any very low frequency signals (below 0.009Hz), which may be due to intrinsic scanner instabilities\n",
    "\n",
    "#### Drop dummy TRs\n",
    "During the initial stages of a functional scan there is a strong signal decay artifact, thus the first 4ish or so \n",
    "TRs are very high intensity signals that don't reflect the rest of the scan. Therefore we drop these timepoints. \n",
    "\n",
    "#### Censoring + Interpolation (leaving out)\n",
    "Censoring involves removal and interpolation of high-movement frames from the fMRI data. Interpolation is typically done using sophisticated algorithms much like [Power et al. 2014](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3849338/). \n",
    "\n",
    "** We won't be using censoring + interpolation since its fairly complicated and would take up too much time **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Confound variables for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing temporal derivatives for confound variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select confounds and view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute temporal derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have noticed, we have NaN's in our {confound}_dt. This happens because there is no prior value to the first index to take a difference with, but this isn't a problem since we're going to be dropping 4 timepoints from our data and confounders anyway!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dummy TR Drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we'll load in our data and check the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the fourth dimension represents frames/TRs(timepoints). We want to drop the first four timepoints entirely, to do so we use nibabel's slicer feature. We'll also drop the first 4 confound variable timepoints to match the functional scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TR drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confound TR drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying confound regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to clean our data of our selected confound variables. There are two ways to go about this: \n",
    "\n",
    "1. If you have nilearn version 0.5.0 or higher use <code>nilearn.image.clean_img(image,confounds,...)</code>\n",
    "2. If you want full control over specific parts of the image you're cleaning use <code>nilearn.signal.clean(signals,confounds,...)</code> \n",
    "\n",
    "The first method is probably most practical and can be done in one line given what we've already set-up. However, in cases of very large datasets (HCP-style), the second method might be preferable for optimizing memory usage. \n",
    "\n",
    "We'll go over both\n",
    "***\n",
    "\n",
    "First note that both methods take an argument <code>confounds</code>. This is a matrix:\n",
    "\n",
    "$$\n",
    "\\left.\\left( \n",
    "\\vphantom{ \\begin{array}{c} 1 \\\\ 1 \\\\1 \\\\1 \\\\1 \\end{array} }\n",
    "\\smash{ \\underbrace{\n",
    "                    \\begin{array}{cccccc} \n",
    "                    a_1 & b_1 & c_1 & \\cdots & x_1 & \\\\\n",
    "                    a_2 & b_2 & c_2 & \\cdots & x_2 &\\\\\n",
    "                    a_3 & b_3 & c_3 & \\cdots & x_3 &\\\\\n",
    "                    \\vdots & \\vdots & \\vdots & \\dots & \\vdots &\\\\\n",
    "                    a_T & b_T & c_T & \\cdots & x_T & \n",
    "                    \\end{array}\n",
    "                   }_{ \\text{ # of confound variables }}\n",
    "      }\n",
    "\\right)\n",
    "\\right\\}\\,T\\text{ number of frames}\n",
    "$$\n",
    "<br></br>\n",
    "\n",
    "Therefore our goal is to take our confound matrix and work it into a matrix of the form above. The end goal is a matrix with 147 rows, and columns matching the number of confound variables (9x2=18)\n",
    "\n",
    "Luckily this is a one-liner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Form a confound matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean our image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 1: Using <code>nilearn.image.clean_img</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll describe a couple of this function's important arguments. Any argument enclosed in [arg] is optional\n",
    "\n",
    "<code>nilearn.image.clean_img(image,confounds,[low_pass],[high_pass],[t_r],[mask_img],[detrend],[standardize])</code>\n",
    "\n",
    "**Required**:\n",
    "- <code>image</code>: The functional image (<code> func_img </code>)\n",
    "- <code>confounds</code>: The confound matrix (<code> confounds </code>) \n",
    "\n",
    "**Optional**:\n",
    "- <code>low_pass</code>: A low pass filter cut-off\n",
    "- <code>high_pass</code> A high pass filter cut-off\n",
    "- <code>t_r</code>: This is required if using low/high pass, the repetition time of acquisition (imaging metadata) \n",
    "- <code>mask_img</code> Apply a mask when performing confound regression, will speed up regression\n",
    "- <code>detrend</code>: Remove drift from the data (useful for removing scanner instability artifacts) [default=True]\n",
    "- <code>standardize</code>: Set mean to 0, and variance to 1 --> sets up data for statistical analysis [default=True]\n",
    "*** \n",
    "**What we're using**: \n",
    "\n",
    "The Repetition Time of our data is 2 seconds, in addition since we're replicating (mostly) Yeo 2011's analysis: \n",
    "- high_pass = 0.009\n",
    "- low_pass = 0.08\n",
    "- detrend = True\n",
    "- standardize = True\n",
    "\n",
    "In addition we'll use a mask of our MNI transformed functional image (<code> mask </code>) to speed up cleaning \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set some constants\n",
    "high_pass= 0.009\n",
    "low_pass = 0.08\n",
    "t_r = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's visualize our result on slice #51"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method 2: Using <code>nilearn.signal.clean</code>\n",
    "\n",
    "The arguments to this function are almost identical to <code>nilearn.image.clean_img</code>: \n",
    "\n",
    "<code>nilearn.signal.clean(signals,confounds,[low_pass],[high_pass],[t_r],[detrend],[standardize]</code> \n",
    "\n",
    "The only difference being:\n",
    "\n",
    "- <code>signals</code>: The resting state signals matrix\n",
    "- no <code>mask_img</code> argument exists, we'll have to pick which voxels to apply confound regression to ourselves!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in nilearn.signal\n",
    "from nilearn import signal as sgl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Set up our data signals matrix\n",
    "Recall our data is a 4D array, with the fourth dimension represented as time and the other 3 dimensions representing the (x,y,z) coordinate of a particular voxel. We want to convert this to a matrix represented as the following:\n",
    "\n",
    "$$\n",
    "\\left.\\left( \n",
    "\\vphantom{ \\begin{array}{c} 1 \\\\ 1 \\\\1 \\\\1 \\\\1 \\end{array} }\n",
    "\\smash{ \\underbrace{\n",
    "                    \\begin{array}{cccccc} \n",
    "                    a_1 & b_1 & c_1 & \\cdots & x_1 & \\\\\n",
    "                    a_2 & b_2 & c_2 & \\cdots & x_2 &\\\\\n",
    "                    a_3 & b_3 & c_3 & \\cdots & x_3 &\\\\\n",
    "                    \\vdots & \\vdots & \\vdots &\\cdots & \\vdots &\\\\\n",
    "                    a_T & b_T & c_T & \\cdots & x_T & \n",
    "                    \\end{array}\n",
    "                   }_{x*y*z \\text{ voxels }}\n",
    "      }\n",
    "\\right)\n",
    "\\right\\}\\,T\\text{ number of frames}\n",
    "$$\n",
    "\n",
    "<br><br>\n",
    "- The **number of columns represents the total number of voxels (x\\*y\\*z)**, each column being a single voxel\n",
    "- The **number of rows represents the number of timepoints**\n",
    "\n",
    "So we need to *reshape* our data to match this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we pull out our data as a numpy arrays\n",
    "\n",
    "\n",
    "#Then we get x,y,z dimensions\n",
    "\n",
    "\n",
    "#Then we get total number of voxels across all frames, x*y*z\n",
    "\n",
    "\n",
    "#Then we reshape to the correct size, note that matrix is flipped on its side\n",
    "#Where the number of rows matches the number of voxels instead of time-series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we flip it over (switch columns and rows) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Masking our signals matrix (Optional)\n",
    "\n",
    "Using this <code>signals</code> matrix will work, but we'll also be cleaning *background/non-brain* voxels which may slow down our cleaning if we're cleaning hundreds (or *thousands* in the case of HCP) of images. \n",
    "\n",
    "To speed up the process we should only apply cleaning to the *subset of voxels that belong to the brain*. We can do this by masking out which voxels to apply cleaning to. This is equivalent to using <code>mask_img</code> in **Method 1** except we'll be doing this manually - it'll be slightly more complicated!.\n",
    "\n",
    "To apply the mask to our <code>signals</code>, we want our mask to be in a similar format to our <code>signals</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in mask image\n",
    "\n",
    "\n",
    "#Pull out data matrix\n",
    "\n",
    "\n",
    "#Get dimensions of mask image\n",
    "\n",
    "\n",
    "#Reshape the data so that each column corresponds to a voxel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is a 1-dimensional array where each element corresponds to a voxel. Any element that is equal to 0 corresponds to a background voxel and any element corresponding to a brain voxel is equal to 1. \n",
    "To select which voxels to clean we'll find all the indices where <code>flattened_mask</code> equals 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the voxel indices (corresponding to column #s in our signals) that are non-zero (brain voxels) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Cleaning our data\n",
    "First we'll set up our filtering variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up variables for confound regression\n",
    "low_pass = 0.08 \n",
    "high_pass = 0.009\n",
    "rep_time= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the actual cleaning once our data is set up will be very similar to <code>nilearn.image.clean_img</code> in form. The major difference will be that to mask our data we'll pick which indices to apply <code>nilearn.signal.clean</code> to using <code>brain_voxels</code>. \n",
    "\n",
    "*** \n",
    "\n",
    "In practice we'll do the following: \n",
    "1. Create an matrix of zeros matching our <code>signals</code>, we'll call it <code>cleaned_signals</code>\n",
    "2. In the voxels (columns) corresponding to <code>brain_voxels</code> write in the cleaned time-series\n",
    "\n",
    "Specifically, step 2 will be accomplished using the following:\n",
    "\n",
    "<code>cleaned_signals[:,brain_voxels] = nilearn.signal.clean(signals[:,brain_voxels],...)</code> \n",
    "\n",
    "Notice, <code>signals[:,brain_voxels]</code>, this does two things: \n",
    "1. Select all rows - **rows correspond to frames and we want all frames**\n",
    "2. Select the columns using <code>brain_voxels</code>. Remember **columns represent voxels, and <code>brain_voxels</code> corresponds to brain voxels**. \n",
    "\n",
    "Then <code>cleaned_signals[:,brain_voxels]</code> will select which voxels to write our cleaned time-series into. Notice that doing this sets our background voxels to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First create a matrix of zeroes that matches our signals matrix\n",
    "\n",
    "\n",
    "#Apply only to brain voxels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can reconstruct our volume, just perform operations in reverse\n",
    "#Step 1: Flip it back\n",
    "\n",
    "\n",
    "#Step 2: Reshape it back into a 4D time-series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The cleaned image generated from nilearn.signal.clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare this to <code>cleaned_img</code> which is what we generated using <code>nilearn.image.clean_img</code>. They should be identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done!\n",
    "\n",
    "Hopefully by now you've learned what confound regression is, and how to perform it in nilearn using 2 different methods. We'd like to note that there are many more methods to perform confound regression (simultaneous signal extraction + confound regression for example) but all those methods fundamentally rely on what you've done here. \n",
    "\n",
    "In addition, performing confound regression on *functional volumes*, is also not the only way to do data cleaning. More modern methods involve applying confound regression on *functional surfaces*, however, those methods are too advanced for an introductory course to functional data analysis and involve tools outside of python. \n",
    "\n",
    "If you're interested in surface-based analysis we recommend that you check out the following sources:\n",
    "\n",
    "1. https://edickie.github.io/ciftify/#/\n",
    "2. https://www.humanconnectome.org/software/connectome-workbench\n",
    "3. [The minimal preprocessing pipelines for the Human Connectome Project](https://www.ncbi.nlm.nih.gov/pubmed/23668970)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
