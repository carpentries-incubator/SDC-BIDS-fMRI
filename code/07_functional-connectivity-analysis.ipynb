{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functional Connectivity Analysis\n",
    "***\n",
    "\n",
    "Now we have an idea of three important components to analyzing neuroimaging data:\n",
    "\n",
    "1. Data manipulation\n",
    "2. Cleaning and confound regression\n",
    "3. Parcellation and signal extraction\n",
    "\n",
    "In this notebook the goal is to integrate these 3 basic components and perform a full analysis of group data using **Intranetwork Functional Connectivity (FC)**. \n",
    "\n",
    "Intranetwork functional connectivity is essentially a result of performing correlational analysis on mean signals extracted from two ROIs. Using this method we can examine how well certain resting state networks, such as the **Default Mode Network (DMN)**, are synchronized across spatially distinct regions. \n",
    "\n",
    "ROI-based correlational analysis forms the basis of many more sophisticated kinds of functional imaging analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Outline\n",
    "***\n",
    "\n",
    "The outline of the notebook is divided into two parts. The first part directly uses what you've learned and builds upon it to perform the final functional connectivity analysis on group data. \n",
    "\n",
    "The second part shows how we can use Nilearn's convenient wrapper functionality to perform the same task with *significantly less effort*. \n",
    "\n",
    "#### Part A: Manual computation \n",
    "1. Functional data cleaning and confound regression\n",
    "2. Applying a parcellation onto the data\n",
    "3. Computing the correlation between two ROI time-series\n",
    "\n",
    "\n",
    "#### Part B: Using Nilearn's high-level features\n",
    "1. Using NiftiLabelsMasker to extract cleaned time-series\n",
    "2. Computing the correlation between two ROI time-series\n",
    "3. Performing analysis on all subjects\n",
    "4. Visualization of final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nilearn import signal as sgl\n",
    "from nilearn import image as img\n",
    "from nilearn import plotting as plot\n",
    "from nilearn import datasets\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fmriprep_dir = '../data/ds000030/derivatives/fmriprep'\n",
    "sub = '10171'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-faefe3124dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Use PyBIDS to parse BIDS data structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBIDSLayout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmriprep_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bids' is not defined"
     ]
    }
   ],
   "source": [
    "#Use PyBIDS to parse BIDS data structure\n",
    "layout = bids.BIDSLayout(fmriprep_dir, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get resting state data (preprocessed, mask, and confounds file)\n",
    "func_files = layout.get(subject=sub, datatype='func', task='rest',  suffix='preproc')\n",
    "mask_files = layout.get(subject=sub, datatype='func', task='rest', suffix='brainmask')\n",
    "confound_files = layout.get(subject=sub, datatype='func', task='rest', suffix='confounds', return_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note that when we use return_type='file' when using <code>layout.get</code> we no longer need to use <code>confound_files.path</code>. We could technically use this on all the files!\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select MNI files\n",
    "func_file = func_files[0].path\n",
    "mask_file = mask_files=[0].path\n",
    "confound_file = confound_files[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Manual Computation of Functional Connectivity\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Cleaning your functional data using filtering, dummy TR removal and confound regression\n",
    "The first step to any functional analysis is to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to help extract our confound regressors from the .tsv file for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Refer to part_06 for code + explanation\n",
    "def extract_confounds(confound_tsv,confounds,dt=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        confound_tsv                    Full path to confounds.tsv\n",
    "        confounds                       A list of confounder variables to extract\n",
    "        dt                              Compute temporal derivatives [default = True]\n",
    "        \n",
    "    Outputs:\n",
    "        confound_mat                    \n",
    "    '''\n",
    "    \n",
    "    #Load in data using Pandas then extract relevant columns\n",
    "    confound_df = pd.read_csv(?,delimiter='\\t') \n",
    "    \n",
    "    #Pick the subset of confounds we're interested in\n",
    "    confound_df = confound_df[?]\n",
    "    \n",
    "    #If using temporal derivatives \n",
    "    if dt:\n",
    "        #For each column create a new column '<colname>_dt' containing the step-wise differences\n",
    "        for col in confound_df.columns:\n",
    "            confound_df['{}_dt'.format(col)] = confound_df[col].diff() \n",
    "    \n",
    "    #Convert into a matrix of values (timepoints)x(variable)\n",
    "    confound_mat = confound_df.values \n",
    "    \n",
    "    #Return confound matrix\n",
    "    return confound_mat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "Let's clean our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings to use:\n",
    "- Confounders: X, Y, Z, RotX, RotY, RotZ, aCompCor01, aCompCor02, Global Signal\n",
    "- Temporal Derivatives: Yes\n",
    "- high_pass = 0.009\n",
    "- low_pass = 0.08\n",
    "- detrend = True\n",
    "- standardize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load functional image\n",
    "tr_drop = 4\n",
    "func_img = img.load_img(func_file)\n",
    "\n",
    "#Extract the first 5 TRs\n",
    "func_img = func_img.slicer[:,:,:,?]\n",
    "\n",
    "#Use the above function to pull out a confound matrix\n",
    "#X, Y, Z, RotX, RotY, RotZ, GlobalSignal, aCompCor01, aCompCor02\n",
    "confounds = extract_confounds(?,?,?)\n",
    "\n",
    "#Drop the first 4 rows of the confounds matrix\n",
    "confounds = confounds[?,:] \n",
    "\n",
    "#Apply image cleaning\n",
    "clean_img = img.clean_img(imgs=??,confounds=??,low_pass=??,high_pass=??,t_r=??,\n",
    "                         mask_img=??) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Setting up the parcellation scheme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply a parcellation we'll have to specify a parcellation to use. \n",
    "For this analysis we'll be using a spatially separated version [Yeo 2011 - 7 Networks](https://www.ncbi.nlm.nih.gov/pubmed/21653723).\n",
    "\n",
    "We chose this parcellation since it nicely characterizes the **DMN**, our network of interest.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yeo_atlas = img.load_img('../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/relabeled_yeo_atlas.nii.gz') \n",
    "\n",
    "#Let's visualize it\n",
    "plot.plot_roi(yeo_atlas,cmap='Paired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the spatially separated Yeo 2011 7 networks and view\n",
    "parcel_file = '../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/relabeled_yeo_atlas.nii.gz' \n",
    "yeo_7 = img.load_img(parcel_file)\n",
    "plot.plot_roi(yeo_7,cmap='Paired',colorbar=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in order to use the parcellation with our functional data it must have the same dimensions. It turns out that the parcellation schema has slightly different dimensions, so we need to resample: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Intra-network functional connectivity analysis\n",
    "Intra-network functional connectivity analysis is determined by computing the correlation between the mean time-series of two spatially distinct regions within the same network. \n",
    "\n",
    "To perform this analysis requires a few simple steps:\n",
    "1. Select 2 ROIs from the same network (DMN) \n",
    "2. Extract the mean time-series from both regions \n",
    "3. Compute the correlation between the two mean ROI time-series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to select two ROIs from the DMN. We've already gone through the hassle of selecting these two regions but many possible combinations exist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select two ROIs and visualize\n",
    "source_ROI = 44\n",
    "target_ROI = 46"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise complete the below tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualize the ROI, we can do this by masking our all values not matching our ROI\n",
    "source_mask = img.math_img('a == {}'.format(source_ROI), a=resamp_yeo7) \n",
    "\n",
    "#Apply the mask to the data\n",
    "masked_source = img.math_img(??,a=??,b=??) \n",
    "\n",
    "#Visualize using plot.plot_roi\n",
    "plot.plot_roi(??) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Do the same for the target ROI\n",
    "target_mask = img.math_img(??.format(??),a=??)\n",
    "masked_target = ??\n",
    "\n",
    "#Plot ROI\n",
    "??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our two regions selected, we'll now extract the mean time-series for each of our two ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Recall that in the parcellation, each voxel is labelled with a number corresponding to a distinct parcel\n",
    "#We want to extract parcels belonging to our ROI.\n",
    "\n",
    "#Pull the voxels indices belonging to the ROI \n",
    "\n",
    "\n",
    "#Get voxel coordinates (x,y,z) list of source and target ROIs\n",
    "\n",
    "\n",
    "#Load up functional data to extract ROI voxels from\n",
    "\n",
    "\n",
    "#Extract the list of voxel time-series belonging to each ROI\n",
    "#This is now a (roi voxel)x(timepoints) array\n",
    "\n",
    "\n",
    "#We want to compute the mean timeseries of each list of voxels (source and target) \n",
    "#This will be a (1) x (timepoints) vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've extracted two average time-series, one from the source, and one from the target.\n",
    "The last step is to compute the correlation between them. This will tell us how well the average time-series from the two DMN regions synchronize with each other.\n",
    "\n",
    "To do this we use <code>np.corrcoef(x,y)</code> which returns a matrix of form:\n",
    "\n",
    "$$ \\rho=\n",
    "\\left( \\begin{matrix}\n",
    "\\rho_{1,1} & \\rho_{1,2} \\\\\n",
    "\\rho_{2,1} & \\rho_{2,2}\n",
    "\\end{matrix} \\right)\n",
    "$$\n",
    "\n",
    "The diagonals represent the correlation of signals with themselves. These are always $1$. The off-diagonal represents the correlation of one signal with another is exactly what we want. In addition the matrix is symmetric so: $\\rho_{1,2} = \\rho_{2,1}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compute correlation and pull the value in the first row, second column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Using nilearn's label masker to extract the timeseries\n",
    "***\n",
    "nilearn has a built in function for extracting timeseries from functional files and doing all the extra signal processing at the same time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import a package from <code>nilearn</code>, called <code>input_data</code> which allows us to pull data using the parcellation file, and at the same time applying data cleaning!\n",
    "\n",
    "We first create an object using the parcellation file <code>yeo_7</code> and our cleaning settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nilearn import input_data\n",
    "\n",
    "masker = input_data.NiftiLabelsMasker(labels_img=yeo_atlas,\n",
    "                                      standardize=True,\n",
    "                                      memory='nilearn_cache',\n",
    "                                      verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object <code>masker</code> is now able to be used on *any functional image of the same size*. What it means to *use the masker* is that you can automatically *apply a parcellation and extract data at the same time*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using nilearn's ConnectivityMeasure to calculate our correlation matrix\n",
    "\n",
    "The second step is to compute the functional connectivity (correlation) matrix. When we use <code>masker</code>, we can compute the correlation matrix *between all ROIs in our parcellation atlas at the same time*. Below we'll show an example on how to use the <code>masker</code> in order to compute correlations on our data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll automatically clean and average data for each of our ROIs at the same time. This is done using <code>masker.fit_transform</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using this data we can calculate a *full correlation matrix* - this is the correlation between *all pairs of ROIs* in our parcellation scheme! We'll use another nilearn tool called <code>ConnectivityMeasure</code> from <code>nilearn.connectome</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the masker, we need to make an object that will calculate connectivity for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use <code>correlation_measure.fit_transform()</code> in order to calculate the full correlation matrix for our parcellated data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Note that we're using a list <code>[cleaned_and_averaged_time_series]</code>, this is becasue <code>correlation_measure</code> works on a *list of subjects*. We'll take advantage of this later!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix which has:\n",
    "\n",
    "- A number of rows matching the number of ROIs in our parcellation atlas\n",
    "- A number of columns, that also matches the number of ROIs in our parcellation atlas\n",
    "\n",
    "You can read this correlation matrix as follows:\n",
    "\n",
    "- Suppose we wanted to know the correlation between ROI 30 and ROI 40\n",
    "- Then Row 30, Column 40 gives us this correlation. \n",
    "- Row 40, Column 40 can also give us this correlation\n",
    "\n",
    "This is because the correlation of $A \\to B = B \\to A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Now try calculating correlation matrices for all subjects in our dataset!\n",
    "\n",
    "We've provided some skeleton code to handle some of the logic. Try to fill in the blanks to the best of your ability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = layout.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_subjects = []\n",
    "ctrl_subjects = []\n",
    "schz_subjects = []\n",
    "\n",
    "#Which confound variables should we use?\n",
    "confound_variables = [??]\n",
    "for sub in subjects:\n",
    "    \n",
    "    #Get the functional file for the subject (MNI space)\n",
    "    func_file = layout.get(subject=?, datatype=??, task=??,\n",
    "                           suffix='preproc', return_type='file')[0]\n",
    "    \n",
    "    #Get the confounds file for the subject (MNI space)\n",
    "    confound_file=layout.get(subject=?, datatype=??, task=??,\n",
    "                             suffix='confounds', return_type='file')[0]\n",
    "    \n",
    "    #Load the functional file in\n",
    "    func_img = img.load_img(func_file)\n",
    "    \n",
    "    #Drop the first 4 TRs\n",
    "    func_img = func_img.slicer[:,:,:,??]\n",
    "    \n",
    "    #Extract the confound variables using the function\n",
    "    confounds = extract_confounds(confound_file,confound_variables)\n",
    "    \n",
    "    #Drop the first 4 rows from the confound matrix\n",
    "    #Which rows and columns should we keep?\n",
    "    confounds = confounds[??,??]\n",
    "    \n",
    "    #Apply the parcellation + cleaning to our data\n",
    "    #What function of masker is used to clean and average data?\n",
    "    time_series = masker.??(??,??)\n",
    "    \n",
    "    #This collects a list of all subjects\n",
    "    pooled_subjects.append(time_series)\n",
    "    \n",
    "    \n",
    "    #If the subject ID starts with a \"1\" then they are control\n",
    "    if sub.startswith('1'):\n",
    "        ctrl_subjects.append(time_series)\n",
    "\n",
    "    #If the subejct ID starts with a \"5\" then they are case (case of schizophrenia)\n",
    "    if sub.startswith('5'):\n",
    "        schz_subjects.append(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helpful trick is that we can re-use the <code>correlation_measure</code> object we made earlier and apply it to a *list of subject data*! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have correlation matrices for each subject across two populations. The final step is to examine the differences between these groups in their correlation between ROI 43 and ROI 45:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrices and Group Differences\n",
    "\n",
    "An important step in any analysis is visualizing the data that we have. We've cleaned data, averaged data and calculated correlations but we don't actually know what it looks like! Visualizing data is important to ensure that we don't throw pure nonsense into our final statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize data we'll be using a python package called <code>seaborn</code> which will allow us to create statistical visualizations with not much effort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a single subject's correlation matrix by using <code>seaborn</code>'s <code>heatmap</code> function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pull our ROI 43 and 45 by indexing our list of correlation matrices as if it were a 3D array (kind of like an MR volume). Take a look at the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of form:\n",
    "\n",
    "```python\n",
    "ctrl_correlation_matrices[subject_index, row_index, column_index]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Try pulling out:\n",
    "\n",
    "- All subjects\n",
    "- Only row 43 (corresponding to ROI 43)\n",
    "- Only column 45 (corresponding to ROI 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_roi_vec = ctrl_correlation_matrices[??,??,??]\n",
    "schz_roi_vec = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
