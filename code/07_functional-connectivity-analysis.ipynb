{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Now we have an idea of three important components to analyzing neuroimaging data:\n",
    "\n",
    "1. Data manipulation\n",
    "2. Cleaning and confound regression\n",
    "3. Parcellation and signal extraction\n",
    "\n",
    "In this notebook the goal is to integrate these 3 basic components and perform a full analysis of group data using **Intranetwork Functional Connectivity (FC)**.\n",
    "\n",
    "Intranetwork functional connectivity is essentially a result of performing correlational analysis on mean signals extracted from two ROIs. Using this method we can examine how well certain resting state networks, such as the **Default Mode Network (DMN)**, are synchronized across spatially distinct regions.\n",
    "\n",
    "ROI-based correlational analysis forms the basis of many more sophisticated kinds of functional imaging analysis.\n",
    "\n",
    "## Using Nilearn's High-level functionality to compute correlation matrices\n",
    "\n",
    "Nilearn has a built in function for extracting timeseries from functional files and doing all the extra signal processing at the same time. Let's walk through how this is done\n",
    "\n",
    "First we'll grab our imports as usual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image as nimg\n",
    "from nilearn import plotting as nplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bids import BIDSLayout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the data that we want to perform our connectivity analysis on using PyBIDS:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use PyBIDS to parse BIDS data structure\n",
    "fmriprep_dir = \"../data/ds000030/derivatives/fmriprep/\"\n",
    "layout = BIDSLayout(fmriprep_dir,\n",
    "                   config=['bids','derivatives'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get resting state data (preprocessed, mask, and confounds file)\n",
    "func_files = layout.get(datatype='func', task='rest',\n",
    "                        desc='preproc',\n",
    "                        space='MNI152NLin2009cAsym',\n",
    "                        extension='nii.gz',\n",
    "                        return_type='file')\n",
    "\n",
    "mask_files = layout.get(datatype='func', task='rest',\n",
    "                        desc='brain',\n",
    "                        suffix=\"mask\",\n",
    "                        space='MNI152NLin2009cAsym',\n",
    "                        extension='nii.gz',\n",
    "                        return_type='file')\n",
    "\n",
    "confound_files = layout.get(datatype='func',\n",
    "                            task='rest',\n",
    "                            desc='confounds',\n",
    "                            extension='tsv',\n",
    "                            return_type='file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of subjects to peform our analysis on, let's load up our parcellation template file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load separated parcellation\n",
    "parcel_file = '../resources/rois/yeo_2011/Yeo_JNeurophysiol11_MNI152/relabeled_yeo_atlas.nii.gz'\n",
    "yeo_7 = nimg.load_img(parcel_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import a package from <code>nilearn</code>, called <code>input_data</code> which allows us to pull data using the parcellation file, and at the same time applying data cleaning!\n",
    "\n",
    "We first create an object using the parcellation file <code>yeo_7</code> and our cleaning settings which are the following:\n",
    "\n",
    "Settings to use:\n",
    "- Confounders: X, Y, Z, RotX, RotY, RotZ, aCompCor01, aCompCor02, Global Signal\n",
    "- Temporal Derivatives: Yes\n",
    "- high_pass = 0.009\n",
    "- low_pass = 0.08\n",
    "- detrend = True\n",
    "- standardize = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object <code>masker</code> is now able to be used on *any functional image of the same size*. The `input_data.NiftiLabelsMasker` object is a wrapper that applies parcellation, cleaning and averaging to an functional image. For example let's apply this to our first subject:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go ahead and start using the <code>masker</code> that we've created, we have to do some preparatory steps. The following should be done prior to use the <code>masker</code> object:\n",
    "1. Make your confounds matrix (as we've done in Episode 06)\n",
    "2. Drop Dummy TRs that are to be excluded from our cleaning, parcellation, and averaging step\n",
    "\n",
    "To help us with the first part, let's define a function to help extract our confound regressors from the .tsv file for us. Note that we've handled pulling the appropriate `{confounds}_derivative1` columns for you! You just need to supply the base regressors!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Refer to part_06 for code + explanation\n",
    "def extract_confounds(confound_tsv,confounds,dt=True):\n",
    "    '''\n",
    "    Arguments:\n",
    "        confound_tsv                    Full path to confounds.tsv\n",
    "        confounds                       A list of confounder variables to extract\n",
    "        dt                              Compute temporal derivatives [default = True]\n",
    "        \n",
    "    Outputs:\n",
    "        confound_mat                    \n",
    "    '''\n",
    "    \n",
    "    if dt:    \n",
    "        dt_names = ['{}_derivative1'.format(c) for c in confounds]\n",
    "        confounds = confounds + dt_names\n",
    "    \n",
    "    #Load in data using Pandas then extract relevant columns\n",
    "    confound_df = pd.read_csv(confound_tsv,delimiter='\\t') \n",
    "    confound_df = confound_df[confounds]\n",
    "    \n",
    " \n",
    "    #Convert into a matrix of values (timepoints)x(variable)\n",
    "    confound_mat = confound_df.values \n",
    "    \n",
    "    #Return confound matrix\n",
    "    return confound_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we'll set up our image file for confound regression (as we did in Episode 6). To do this we'll drop 4 TRs from *both our functional image and our confounds file*. Note that our <code>masker</code> object will not do this for us!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the masker\n",
    "Finally with everything set up, we can now use the masker to perform our:\n",
    "1. Confounds cleaning\n",
    "2. Parcellation\n",
    "3. Averaging within a parcel\n",
    "All in one step!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to be clear, this data is *automatically parcellated for you, and, in addition, is cleaned using the confounds you've specified!*\n",
    "\n",
    "The result of running <code>masker.fit_transform</code> is a matrix that has:\n",
    "- Rows matching the number of timepoints (148)\n",
    "- Columns, each for one of the ROIs that are extracted (43)\n",
    "\n",
    "**But wait!**\n",
    "\n",
    "We originally had **46 ROIs**, what happened to 3 of them? It turns out that <code>masker</code> drops ROIs that are empty (i.e contain no brain voxels inside of them), this means that 3 of our atlas' parcels did not correspond to any region with signal! To see which ROIs are kept after computing a parcellation you can look at the <code>labels_</code> property of <code>masker</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our ROIs of interest (44 and 46) cannot be accessed using the 44th and 46th columns directly! Instead we'll have to figure out which of the 43 columns correspond to ROI 44 and ROI 46. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that the:\n",
    "- 38th column is ROI 44\n",
    "- 40th column is ROI 46\n",
    "\n",
    "We stored them in the <code>ROI_44</code> and <code>ROI_46</code> variables so we can use these later!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Connectivity\n",
    "\n",
    "In fMRI imaging, connectivity typically refers to the *correlation of the timeseries of 2 ROIs*. Therefore we can calculate a *full connectivity matrix* by computing the correlation between *all pairs of ROIs* in our parcellation scheme! \n",
    "\n",
    "We'll use another nilearn tool called <code>ConnectivityMeasure</code> from <code>nilearn.connectome</code>. This tool will perform the full set of pairwise correlations for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the masker, we need to make an object that will calculate connectivity for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using <code>SHIFT-TAB</code> to see what options you can put into the <code>kind</code> argument of <code>ConnectivityMeasure</code>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use <code>correlation_measure.fit_transform()</code> in order to calculate the full correlation matrix for our parcellated data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using a list <code>[cleaned_and_averaged_time_series]</code>, this is becasue <code>correlation_measure</code> works on a *list of subjects*. We'll take advantage of this later!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a matrix which has:\n",
    "\n",
    "- A number of rows matching the number of ROIs in our parcellated data\n",
    "- A number of columns, that also matches the number of ROIs in our parcellated data\n",
    "\n",
    "Suppose we wanted to know the correlation between ROI 44 and ROI 46. To get this value we need look at the row and column indices that correspond to these ROIs!\n",
    "\n",
    "Recall that since we lost 3 columns, we had to use the <code>masker.labels_</code> property to figure out which column corresponds to which of the atlas' original ROIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the data extract process shown above to all subjects in our subject list and collect the results. Your job is to fill in the blanks!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we're going to create some empty lists to store all our data in!\n",
    "pooled_subjects = []\n",
    "ctrl_subjects = []\n",
    "schz_subjects = []\n",
    "\n",
    "#Which confound variables should we use?\n",
    "confound_variables = ['trans_x','trans_y','trans_z',\n",
    "                               'rot_x','rot_y','rot_z',\n",
    "                               'global_signal',\n",
    "                               'white_matter','csf']\n",
    "\n",
    "# Lets get all the subjects we have\n",
    "subjects = layout.get_subjects()\n",
    "for sub in subjects:\n",
    "    \n",
    "    #Get the functional file for the subject (MNI space)\n",
    "    func_file = layout.get(subject=??,\n",
    "                           datatype='??', task='??',\n",
    "                           desc='preproc',\n",
    "                           extension=\"nii.gz\",\n",
    "                           return_type='file')[0]\n",
    "    \n",
    "    #Get the confounds file for the subject (MNI space)\n",
    "    confound_file=layout.get(subject=??, datatype='??',\n",
    "                             task='??',\n",
    "                             desc='confounds',\n",
    "                             extension='tsv',\n",
    "                             return_type='file')[0]\n",
    "    \n",
    "    #Load the functional file in\n",
    "    func_img = nimg.load_img(func_file)\n",
    "    \n",
    "    #Drop the first 4 TRs\n",
    "    func_img = func_img.slicer[:,:,:,??]\n",
    "    \n",
    "    \n",
    "    #Extract the confound variables using the function\n",
    "    confounds = extract_confounds(confound_file,\n",
    "                                  confound_variables)\n",
    "    \n",
    "    #Drop the first 4 rows from the confound matrix\n",
    "    #Which rows and columns should we keep?\n",
    "    confounds = confounds[??, ??]\n",
    "    \n",
    "    #Apply the parcellation + cleaning to our data\n",
    "    #What function of masker is used to clean and average data?\n",
    "    time_series = masker.??(??, ??)\n",
    "    \n",
    "    #This collects a list of all subjects\n",
    "    pooled_subjects.append(time_series)\n",
    "    \n",
    "    #If the subject ID starts with a \"1\" then they are control\n",
    "    if sub.startswith('1'):\n",
    "        ctrl_subjects.append(time_series)\n",
    "    #If the subject ID starts with a \"5\" then they are case (case of schizophrenia)\n",
    "    if sub.startswith('5'):\n",
    "        schz_subjects.append(time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of all of this code is that:\n",
    "\n",
    "1. Subjects who start with a \"1\" in their ID, are controls, and are placed into the `ctrl_subjects` list\n",
    "2. Subjects who start with a \"2\" in their ID, have schizophrenia, and are placed into the `schz_subjects` list\n",
    "\n",
    "What's actually being placed into the list? The cleaned, parcellated time series data for each subject (the output of <code>masker.fit_transform</code>)!\n",
    "\n",
    "A helpful trick is that we can re-use the <code>correlation_measure</code> object we made earlier and apply it to a *list of subject data*! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have correlation matrices for each subject across two populations. The final step is to examine the differences between these groups in their correlation between ROI 43 and ROI 45.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correlation Matrices and Group Differences\n",
    "\n",
    "An important step in any analysis is visualizing the data that we have. We've cleaned data, averaged data and calculated correlations but we don't actually know what it looks like! Visualizing data is important to ensure that we don't throw pure nonsense into our final statistical analysis\n",
    "\n",
    "To visualize data we'll be using a python package called <code>seaborn</code> which will allow us to create statistical visualizations with not much effort.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view a single subject's correlation matrix by using <code>seaborn</code>'s <code>heatmap</code> function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pull our ROI 44 and 46 by indexing our list of correlation matrices as if it were a 3D array (kind of like an MR volume). Take a look at the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is of form:\n",
    "\n",
    "<code>ctrl_correlation_matrices[subject_index, row_index, column_index]</code>\n",
    "\n",
    "Now we're going to pull out just the correlation values between ROI 43 and 45 *across all our subjects*. This can be done using standard array indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we're going to arrange this data into a table. We'll create two tables (called dataframes in the python package we're using, `pandas`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is:\n",
    "\n",
    "- `ctrl_df` a table containing the correlation value for each control subject, with an additional column with the group label, which is 'control'\n",
    "\n",
    "- `scz_df` a table containing the correlation value for each schizophrenia group subject, with an additional column with the group label, which is 'schizophrenia'\n",
    "\n",
    "For visualization we're going to stack the two tables together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we're going to visualize the results using the python package `seaborn`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the results here aren't significant they seem to indicate that there might be three subclasses in our schizophrenia group - of course we'd need *a lot* more data to confirm this! The interpretation of these results should ideally be based on some *a priori* hypothesis! \n",
    "\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "Hopefully now you understand that:\n",
    "\n",
    "1. fMRI data needs to be pre-processed before analyzing\n",
    "2. Manipulating images in python is easily done using `nilearn` and `nibabel`\n",
    "3. You can also do post-processing like confound/nuisance regression using `nilearn`\n",
    "4. Parcellating is a method of simplifying and \"averaging\" data. The type of parcellation reflect assumptions you make about the structure of your data\n",
    "5. Functional Connectivity is really just time-series correlations between two signals!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
